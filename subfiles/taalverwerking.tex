\documentclass[../main.tex]{subfiles}
\begin{document}

\subsection{Evolutie \& Innovaties}
\begin{question}
Schets de evolutie van de natuurlijke taalverwerking over de voorbije decennia aan de hand van belangrijke innovaties. Wat zijn de sturende factoren in deze evolutie? Illustreer met voorbeelden.
Zijn er parallelle evoluties waar te nemen in andere domeinen van de informatica?
\end{question}

\subsection{Automatisch Vertalen}
\begin{question}
Schets de evolutie van het \textbf{automatisch vertalen} van de natuurlijke taal over de voorbije decennia aan de hand van belangrijke innovaties.
Wat zijn de sturende factoren in deze evolutie?
Illustreer met voorbeelden.
\end{question}
\begin{solution}
	\item Het hoewel automatisch vertalen is al lang een droom blijven doorbraken uit tot na de tweede wereldoorlog. In de 17e eeuw beschikte men al over talen om correct te redeneren en werd een mechanisch woordenboek gesuggereerd.
	\item Warren Weaver suggereert in 1949 om statistiek en cryptografische technieken uit de tweede wereldoorlog toe te passen om de onderliggende logica en universele kenmerken van de taal te exploreren.
	\item De \textbf{koude oorlog} zorgt ervoor dat zowel in de VS als in Rusland de aandacht voor het automatisch vertalen van en naar het Russisch toeneemt. De oprichting van de \textbf{Europese Economische Gemeenschap} in 1957 creëert noodzaak voor automatisch met onderzoek in verschillende Europese universiteit tot gevolg.
	\item In 1967 verschijnt een vernietigend rapport van van de \textbf{ALPAC} (Automatic Language Processing Advisory Committee) dat stelt dat automatisch vertalen te veel \emph{post-processing} vraagt. Dit leidt tot een stille decade.
	\item In 1978 ging het ambitieuze \textbf{Eurotra} project van de Europese Commissie van start. Theoretisch lingu\"istisch en computationeel lingu\"istisch onderzoek met kennisgebaseerde en interlingua technieken. Er werd echter niet in geslaagd een werkend prototype te leveren en de industri\"ele partnerschappen wierpen geen vruchten af.
	\item Eind jaren `80 onstaat \textbf{Statistical Machine Translation} (SMT). Dit maakt gebruik van het aligneren van woorden, frasen en zinnen in parallel corpus en het toepassen van geavanaceerde statische methoden hierop. Een voorbeeld van dergelijke corpus zijn de Canadese parlementaire documenten (Engels-Frans). Peter Brown (IBM) publiceert deen dergelijk systeem 1988 wat resulteert in het zeer invloedrijke \textbf{Candide} systeem. De komende jaren volgen nog verschillende toolkits (\textbf{EGYPT}, \textbf{Moses}). Een grote mijlpaal is het commercieel statistisch MT systeem van Google in 2007.
\end{solution}

\subsection{Automatisch Verstaan}
\begin{question}
Schets de evolutie van het \textbf{automatisch verstaan} van de natuurlijke taal over de voorbije decennia aan de hand van belangrijke innovaties.
Wat zijn de sturende factoren in deze evolutie?
Illustreer met voorbeelden.
\end{question}

\subsection{Multidisciplinair Onderzoeksgebied}
\begin{question}
Leg uit waarom natuurlijke taalverwerking als een multidisciplinair onderzoeksgebied wordt beschouwd.
\end{question}

\subsection{Empirisme}
\begin{question}
Wat is empirisme? Verklaar aan de hand van een voorbeeld uit de natuurlijke taalverwerking.
\end{question}

\begin{solution}
Empirisme is de veronderstelling dat kennis voortkomt uit proefondervindelijke ervaringen.
Vanuit het standpunt van natuurlijke taalverwerking betekent dit dat de focus ligt op data-gedreven modellen die worden gevalideerd aan de hand van \emph{held-out} datasets.
Het schoolvoorbeeld is \emph{IBM Watson}, een computer die in spreektaal gestelde vragen beantwoordt aan de hand van een verzameling (on)gestructureerde informatie.
\end{solution}

\subsection{Noam Chomsky}
\begin{question}
Wat zijn de bijdragen van Noam Chomsky voor de natuurlijke taalverwerking?
Evalueer kritisch deze bijdragen.
\end{question}

\begin{solution}
Noam Chomsky schreef als professor aan MIT verschillende invloedrijke artikels die leidde tot de conclusie dat statische modellen niet in staat zijn om een volledig cognitief model van menselijke grammaticale kennis te genereren.
Zijn theorie veronderstelt dat taalvermogen is aangeboren.
Hij staat vooral gekend voor zijn bijdrage tot de formele taaltheorie met het ontwikkelen van \textbf{contextvrije grammatica} en \textbf{generatieve grammatica}. Deze laatste beïnvloedde onder meer \emph{head-driven phrase structure grammar}, \emph{generalized phrase structure grammar}, \emph{lexical functional grammar} en \emph{combinatory categorial grammar}.
\end{solution}

\subsection{IBM \& Google}
\begin{question}
Wat is de rol van grote bedrijven zoals IBM of Google in de ontwikkeling van taaltechnologie in het verleden en vandaag?
Welke voordelen hebben deze bedrijven in vergelijking met kennisinstellingen?
\end{question}

\end{document}